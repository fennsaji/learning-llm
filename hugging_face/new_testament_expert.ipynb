{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Testament Expert using Hugging Face Models\n",
    "\n",
    "This notebook demonstrates how to create a New Testament expert using Hugging Face transformer models specialized in biblical knowledge."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install transformers torch accelerate gradio"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport gradio as gr\nimport threading\nimport time"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using Text Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper for New Testament questions\n",
    "nt_pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"sleepdeprived3/Christian-Bible-Expert-v2.0-12B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# New Testament specific questions\n",
    "nt_questions = [\n",
    "    \"Who were the twelve apostles of Jesus?\",\n",
    "    \"What is the significance of the Sermon on the Mount?\",\n",
    "    \"Explain the parable of the Good Samaritan\",\n",
    "    \"What happened during the Last Supper?\",\n",
    "    \"Describe the resurrection of Jesus Christ\"\n",
    "]\n",
    "\n",
    "for question in nt_questions:\n",
    "    print(f\"\\nüîπ Question: {question}\")\n",
    "    result = nt_pipe(\n",
    "        f\"New Testament Question: {question}\\nAnswer:\",\n",
    "        max_length=200,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=nt_pipe.tokenizer.eos_token_id\n",
    "    )\n",
    "    print(f\"üìñ Answer: {result[0]['generated_text'].split('Answer:')[1].strip()}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Interactive New Testament Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewTestamentExpert:\n",
    "    def __init__(self, model_name=\"sleepdeprived3/Christian-Bible-Expert-v2.0-12B\"):\n",
    "        print(\"Loading New Testament Expert model...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Set pad token if not available\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"‚úÖ New Testament Expert model loaded successfully!\")\n",
    "    \n",
    "    def ask_question(self, question, max_length=300):\n",
    "        \"\"\"\n",
    "        Ask a New Testament related question\n",
    "        \"\"\"\n",
    "        # Format the prompt specifically for New Testament queries\n",
    "        prompt = f\"\"\"You are a New Testament expert. Please provide a detailed and accurate answer based on the New Testament scriptures.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode and clean the response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = response.split(\"Answer:\")[1].strip() if \"Answer:\" in response else response\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def get_verse_explanation(self, verse_reference):\n",
    "        \"\"\"\n",
    "        Get explanation for a specific New Testament verse\n",
    "        \"\"\"\n",
    "        question = f\"Please explain the meaning and context of {verse_reference} from the New Testament.\"\n",
    "        return self.ask_question(question)\n",
    "    \n",
    "    def compare_gospels(self, topic):\n",
    "        \"\"\"\n",
    "        Compare how different gospels present a topic\n",
    "        \"\"\"\n",
    "        question = f\"How do the four gospels (Matthew, Mark, Luke, and John) present {topic}? What are the similarities and differences?\"\n",
    "        return self.ask_question(question, max_length=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the New Testament Expert\n",
    "nt_expert = NewTestamentExpert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the New Testament Expert with various questions\n",
    "questions = [\n",
    "    \"What are the Beatitudes and their significance?\",\n",
    "    \"Who was Paul the Apostle and what were his main contributions?\",\n",
    "    \"What is the Book of Revelation about?\",\n",
    "    \"Explain the concept of salvation in the New Testament\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nüîπ Question: {question}\")\n",
    "    answer = nt_expert.ask_question(question)\n",
    "    print(f\"üìñ Answer: {answer}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Verse Explanation and Gospel Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain specific New Testament verses\n",
    "verses = [\n",
    "    \"John 3:16\",\n",
    "    \"Romans 8:28\",\n",
    "    \"Philippians 4:13\",\n",
    "    \"1 Corinthians 13:4-7\"\n",
    "]\n",
    "\n",
    "print(\"üìú NEW TESTAMENT VERSE EXPLANATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for verse in verses:\n",
    "    print(f\"\\nüìñ Verse: {verse}\")\n",
    "    explanation = nt_expert.get_verse_explanation(verse)\n",
    "    print(f\"üí° Explanation: {explanation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare how different gospels present various topics\n",
    "topics = [\n",
    "    \"the birth of Jesus\",\n",
    "    \"the crucifixion\",\n",
    "    \"the resurrection\"\n",
    "]\n",
    "\n",
    "print(\"‚öñÔ∏è GOSPEL COMPARISONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\nüîç Topic: {topic.title()}\")\n",
    "    comparison = nt_expert.compare_gospels(topic)\n",
    "    print(f\"üìä Comparison: {comparison}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 4: Interactive Q&A Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive session (uncomment to use)\n",
    "# print(\"üî• New Testament Expert - Interactive Session\")\n",
    "# print(\"Ask any question about the New Testament. Type 'exit' to quit.\\n\")\n",
    "\n",
    "# while True:\n",
    "#     user_question = input(\"‚ùì Your question: \")\n",
    "#     \n",
    "#     if user_question.lower() in ['exit', 'quit', 'bye']:\n",
    "#         print(\"üëã Thank you for using the New Testament Expert!\")\n",
    "#         break\n",
    "#     \n",
    "#     if user_question.strip():\n",
    "#         print(\"\\nü§î Thinking...\")\n",
    "#         answer = nt_expert.ask_question(user_question)\n",
    "#         print(f\"\\nüìñ Answer: {answer}\\n\")\n",
    "#         print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 5: Using Different Biblical Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Alternative biblical models available on Hugging Face\nbiblical_models = {\n    \"sleepdeprived3/Christian-Bible-Expert-v2.0-12B\": \"Primary biblical expert model\",\n    \"sleepdeprived3/Reformed-Christian-Bible-Expert-12B_EXL2_5bpw_H8\": \"Reformed/Calvinist theological perspective\",\n    \"aao331/ChristGPT-13B-V2-GPTQ\": \"Instruction-tuned model trained on bible to act like Jesus\",\n    \"oliverbob/openbible\": \"Open biblical knowledge model\",\n    \"abhi1nandy2/Bible-roberta-base\": \"RoBERTa model trained on biblical text\",\n    \"Buttsac/bible\": \"Bible-focused language model\",\n}\n\nprint(\"üìö AVAILABLE BIBLICAL MODELS:\")\nprint(\"=\" * 60)\nfor model, description in biblical_models.items():\n    print(f\"üîπ {model}\")\n    print(f\"   ‚îî‚îÄ {description}\")\n    print()\n\nprint(\"üí° For optimal New Testament expertise, consider:\")\nprint(\"1. Fine-tuning a model on New Testament text\")\nprint(\"2. Using retrieval-augmented generation (RAG) with New Testament corpus\")\nprint(\"3. Implementing semantic search over New Testament verses\")\nprint(\"4. Using the bible-nlp organization's datasets and tools\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Method 6: Gradio Chat Interface\n\nInteractive web-based chat interface for the New Testament Expert using Gradio.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GradioNewTestamentExpert:\n    def __init__(self, model_name=\"sleepdeprived3/Christian-Bible-Expert-v2.0-12B\"):\n        self.model_name = model_name\n        self.expert = None\n        self.initialize_model()\n    \n    def initialize_model(self):\n        \"\"\"Initialize the model with proper error handling\"\"\"\n        try:\n            print(f\"üîÑ Loading {self.model_name}...\")\n            self.expert = NewTestamentExpert(self.model_name)\n            print(\"‚úÖ Model loaded successfully!\")\n        except Exception as e:\n            print(f\"‚ùå Error loading model: {e}\")\n            print(\"üîÑ Falling back to pipeline approach...\")\n            self.expert = pipeline(\n                \"text-generation\",\n                model=self.model_name,\n                torch_dtype=torch.float16,\n                device_map=\"auto\"\n            )\n    \n    def chat_response(self, message, history):\n        \"\"\"Generate response for Gradio chat interface\"\"\"\n        if not message.strip():\n            return \"Please ask a question about the New Testament.\"\n        \n        try:\n            # Add context for New Testament focus\n            nt_prompt = f\"\"\"You are a New Testament expert. Please provide a detailed and accurate answer based on the New Testament scriptures.\n\nQuestion: {message}\nAnswer:\"\"\"\n            \n            if hasattr(self.expert, 'ask_question'):\n                # Using the expert class\n                response = self.expert.ask_question(message)\n            else:\n                # Using pipeline fallback\n                result = self.expert(\n                    nt_prompt,\n                    max_length=300,\n                    num_return_sequences=1,\n                    temperature=0.7,\n                    do_sample=True,\n                    pad_token_id=self.expert.tokenizer.eos_token_id\n                )\n                response = result[0]['generated_text'].split('Answer:')[1].strip()\n            \n            return response\n            \n        except Exception as e:\n            return f\"Sorry, I encountered an error: {str(e)}. Please try rephrasing your question.\"\n    \n    def launch_chat(self, share=False):\n        \"\"\"Launch the Gradio chat interface\"\"\"\n        \n        # Custom CSS for better styling\n        css = \"\"\"\n        .gradio-container {\n            font-family: 'Arial', sans-serif;\n        }\n        .chat-message {\n            padding: 10px;\n            margin: 5px;\n            border-radius: 10px;\n        }\n        \"\"\"\n        \n        # Create the Gradio interface\n        with gr.Blocks(css=css, title=\"New Testament Expert Chat\") as demo:\n            gr.Markdown(\n                \"\"\"\n                # üìñ New Testament Expert Chat\n                \n                Welcome to the New Testament Expert! Ask any questions about:\n                - New Testament books and verses\n                - Jesus' teachings and parables\n                - The apostles and early church\n                - Biblical theology and interpretation\n                - Gospel comparisons\n                \n                **Model**: `sleepdeprived3/Christian-Bible-Expert-v2.0-12B`\n                \"\"\"\n            )\n            \n            chatbot = gr.Chatbot(\n                [],\n                elem_id=\"chatbot\",\n                height=400,\n                bubble_full_width=False\n            )\n            \n            with gr.Row():\n                msg = gr.Textbox(\n                    label=\"Your Question\",\n                    placeholder=\"Ask about the New Testament...\",\n                    container=False,\n                    scale=7\n                )\n                submit_btn = gr.Button(\"Send\", scale=1, variant=\"primary\")\n                clear_btn = gr.Button(\"Clear\", scale=1)\n            \n            # Example questions\n            gr.Examples(\n                examples=[\n                    \"Who were the twelve apostles?\",\n                    \"Explain the parable of the prodigal son\",\n                    \"What is the significance of John 3:16?\",\n                    \"Compare the resurrection accounts in the gospels\",\n                    \"What are the Beatitudes?\",\n                    \"Who was Paul and what were his main teachings?\",\n                    \"Explain the concept of salvation in the New Testament\"\n                ],\n                inputs=msg\n            )\n            \n            def respond(message, chat_history):\n                bot_message = self.chat_response(message, chat_history)\n                chat_history.append((message, bot_message))\n                return \"\", chat_history\n            \n            def clear_chat():\n                return [], \"\"\n            \n            # Event handlers\n            msg.submit(respond, [msg, chatbot], [msg, chatbot])\n            submit_btn.click(respond, [msg, chatbot], [msg, chatbot])\n            clear_btn.click(clear_chat, outputs=[chatbot, msg])\n        \n        print(\"üöÄ Launching New Testament Expert Chat...\")\n        print(\"üì± The interface will open in your browser\")\n        \n        return demo.launch(share=share, debug=True)\n\n# Initialize the Gradio expert\ngradio_expert = GradioNewTestamentExpert()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Launch the Gradio chat interface\n# Set share=True to create a public link that can be shared\ngradio_expert.launch_chat(share=False)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Method 7: Multi-Model Comparison Chat",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class MultiModelBiblicalChat:\n    def __init__(self):\n        self.models = {\n            \"Christian Expert\": \"sleepdeprived3/Christian-Bible-Expert-v2.0-12B\",\n            \"Reformed Expert\": \"sleepdeprived3/Reformed-Christian-Bible-Expert-12B_EXL2_5bpw_H8\",\n            \"ChristGPT\": \"aao331/ChristGPT-13B-V2-GPTQ\"\n        }\n        self.loaded_models = {}\n    \n    def load_model(self, model_name):\n        \"\"\"Load a specific model\"\"\"\n        if model_name not in self.loaded_models:\n            try:\n                print(f\"Loading {model_name}...\")\n                self.loaded_models[model_name] = pipeline(\n                    \"text-generation\",\n                    model=model_name,\n                    torch_dtype=torch.float16,\n                    device_map=\"auto\",\n                    trust_remote_code=True\n                )\n                print(f\"‚úÖ {model_name} loaded successfully!\")\n            except Exception as e:\n                print(f\"‚ùå Failed to load {model_name}: {e}\")\n                return None\n        return self.loaded_models[model_name]\n    \n    def get_response(self, model_name, question):\n        \"\"\"Get response from a specific model\"\"\"\n        model = self.load_model(model_name)\n        if model is None:\n            return f\"Could not load {model_name}\"\n        \n        try:\n            prompt = f\"\"\"You are a New Testament expert. Please provide a detailed and accurate answer based on the New Testament scriptures.\n\nQuestion: {question}\nAnswer:\"\"\"\n            \n            result = model(\n                prompt,\n                max_length=250,\n                num_return_sequences=1,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=model.tokenizer.eos_token_id\n            )\n            \n            response = result[0]['generated_text'].split('Answer:')[1].strip()\n            return response\n            \n        except Exception as e:\n            return f\"Error generating response: {str(e)}\"\n    \n    def create_comparison_interface(self):\n        \"\"\"Create Gradio interface for model comparison\"\"\"\n        \n        def compare_responses(question, selected_models):\n            if not question.strip():\n                return \"Please enter a question.\"\n            \n            results = {}\n            for model_type in selected_models:\n                model_name = self.models[model_type]\n                print(f\"Getting response from {model_type}...\")\n                response = self.get_response(model_name, question)\n                results[model_type] = response\n            \n            return results\n        \n        with gr.Blocks(title=\"Biblical Models Comparison\") as demo:\n            gr.Markdown(\n                \"\"\"\n                # üîç Biblical Models Comparison\n                \n                Compare responses from different biblical AI models to the same New Testament question.\n                Each model may provide different perspectives and insights.\n                \"\"\"\n            )\n            \n            with gr.Row():\n                question_input = gr.Textbox(\n                    label=\"New Testament Question\",\n                    placeholder=\"Enter your biblical question here...\",\n                    lines=2\n                )\n            \n            model_selection = gr.CheckboxGroup(\n                choices=list(self.models.keys()),\n                value=[\"Christian Expert\"],\n                label=\"Select Models to Compare\"\n            )\n            \n            compare_btn = gr.Button(\"Compare Responses üîç\", variant=\"primary\")\n            \n            # Results display\n            with gr.Row():\n                for model_type in self.models.keys():\n                    with gr.Column():\n                        gr.Markdown(f\"### {model_type}\")\n                        gr.Textbox(\n                            label=f\"{model_type} Response\",\n                            lines=8,\n                            interactive=False,\n                            elem_id=f\"{model_type.lower().replace(' ', '_')}_output\"\n                        )\n            \n            # Example questions\n            gr.Examples(\n                examples=[\n                    \"What is the Great Commission?\",\n                    \"Explain the parable of the talents\",\n                    \"What does Paul teach about faith vs works?\",\n                    \"Compare the birth narratives in Matthew and Luke\"\n                ],\n                inputs=question_input\n            )\n            \n            def update_outputs(question, models):\n                results = compare_responses(question, models)\n                outputs = []\n                for model_type in self.models.keys():\n                    if model_type in results:\n                        outputs.append(results[model_type])\n                    else:\n                        outputs.append(\"\")\n                return outputs\n            \n            compare_btn.click(\n                update_outputs,\n                inputs=[question_input, model_selection],\n                outputs=[gr.Textbox(elem_id=f\"{model_type.lower().replace(' ', '_')}_output\") \n                        for model_type in self.models.keys()]\n            )\n        \n        return demo\n\n# Initialize multi-model comparison\nmulti_model_chat = MultiModelBiblicalChat()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Launch the multi-model comparison interface\n# comparison_demo = multi_model_chat.create_comparison_interface()\n# comparison_demo.launch(share=False)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Method 8: Advanced Gradio Features",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_advanced_nt_interface():\n    \"\"\"Advanced New Testament interface with multiple features\"\"\"\n    \n    # Initialize expert\n    expert = NewTestamentExpert()\n    \n    def chat_with_context(message, history, context_type, temperature):\n        \"\"\"Enhanced chat with context selection\"\"\"\n        contexts = {\n            \"General\": \"You are a New Testament expert. Provide accurate biblical answers.\",\n            \"Pastoral\": \"You are a pastor providing pastoral guidance based on the New Testament.\",\n            \"Academic\": \"You are a biblical scholar providing academic analysis of New Testament texts.\",\n            \"Devotional\": \"You are providing devotional insights from the New Testament for spiritual growth.\"\n        }\n        \n        context_prompt = contexts.get(context_type, contexts[\"General\"])\n        full_prompt = f\"{context_prompt}\\n\\nQuestion: {message}\\nAnswer:\"\n        \n        try:\n            # Use the expert's model directly with custom temperature\n            inputs = expert.tokenizer.encode(full_prompt, return_tensors=\"pt\")\n            \n            with torch.no_grad():\n                outputs = expert.model.generate(\n                    inputs,\n                    max_length=350,\n                    num_return_sequences=1,\n                    temperature=temperature,\n                    do_sample=True,\n                    pad_token_id=expert.tokenizer.eos_token_id\n                )\n            \n            response = expert.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            answer = response.split(\"Answer:\")[1].strip() if \"Answer:\" in response else response\n            \n            return answer\n            \n        except Exception as e:\n            return f\"Error: {str(e)}\"\n    \n    def verse_lookup(reference):\n        \"\"\"Look up and explain a Bible verse\"\"\"\n        if not reference.strip():\n            return \"Please enter a verse reference (e.g., John 3:16)\"\n        \n        return expert.get_verse_explanation(reference)\n    \n    def topic_study(topic, verses_to_include):\n        \"\"\"Create a topic study\"\"\"\n        if not topic.strip():\n            return \"Please enter a topic to study\"\n        \n        question = f\"Provide a comprehensive study on {topic} in the New Testament\"\n        if verses_to_include:\n            question += f\", specifically including references to: {verses_to_include}\"\n        \n        return expert.ask_question(question, max_length=500)\n    \n    # Create the interface\n    with gr.Blocks(title=\"Advanced New Testament Expert\", theme=gr.themes.Soft()) as demo:\n        gr.Markdown(\n            \"\"\"\n            # üìñ Advanced New Testament Expert\n            \n            Comprehensive New Testament study tool with multiple interaction modes.\n            \"\"\"\n        )\n        \n        with gr.Tabs():\n            # Chat Tab\n            with gr.TabItem(\"üí¨ Chat\"):\n                with gr.Row():\n                    with gr.Column(scale=3):\n                        chatbot = gr.Chatbot(height=400)\n                        msg = gr.Textbox(\n                            label=\"Your Message\",\n                            placeholder=\"Ask about the New Testament...\"\n                        )\n                        \n                        with gr.Row():\n                            send_btn = gr.Button(\"Send\", variant=\"primary\")\n                            clear_btn = gr.Button(\"Clear\")\n                    \n                    with gr.Column(scale=1):\n                        context_type = gr.Radio(\n                            choices=[\"General\", \"Pastoral\", \"Academic\", \"Devotional\"],\n                            value=\"General\",\n                            label=\"Response Style\"\n                        )\n                        \n                        temperature = gr.Slider(\n                            minimum=0.1,\n                            maximum=1.0,\n                            value=0.7,\n                            step=0.1,\n                            label=\"Creativity Level\"\n                        )\n                        \n                        gr.Markdown(\n                            \"\"\"\n                            **Response Styles:**\n                            - **General**: Standard biblical answers\n                            - **Pastoral**: Guidance and counseling focus\n                            - **Academic**: Scholarly analysis\n                            - **Devotional**: Spiritual growth focus\n                            \"\"\"\n                        )\n                \n                def respond(message, chat_history, context, temp):\n                    bot_message = chat_with_context(message, chat_history, context, temp)\n                    chat_history.append((message, bot_message))\n                    return \"\", chat_history\n                \n                msg.submit(respond, [msg, chatbot, context_type, temperature], [msg, chatbot])\n                send_btn.click(respond, [msg, chatbot, context_type, temperature], [msg, chatbot])\n                clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, msg])\n            \n            # Verse Lookup Tab\n            with gr.TabItem(\"üîç Verse Lookup\"):\n                verse_input = gr.Textbox(\n                    label=\"Verse Reference\",\n                    placeholder=\"e.g., John 3:16, Romans 8:28, Matthew 5:3-12\"\n                )\n                verse_btn = gr.Button(\"Explain Verse\", variant=\"primary\")\n                verse_output = gr.Textbox(\n                    label=\"Explanation\",\n                    lines=10,\n                    interactive=False\n                )\n                \n                verse_btn.click(verse_lookup, inputs=verse_input, outputs=verse_output)\n                \n                gr.Examples(\n                    examples=[\n                        \"John 3:16\",\n                        \"Romans 8:28\",\n                        \"Philippians 4:13\",\n                        \"1 Corinthians 13:4-7\",\n                        \"Ephesians 2:8-9\",\n                        \"Matthew 28:19-20\"\n                    ],\n                    inputs=verse_input\n                )\n            \n            # Topic Study Tab\n            with gr.TabItem(\"üìö Topic Study\"):\n                topic_input = gr.Textbox(\n                    label=\"Topic\",\n                    placeholder=\"e.g., love, forgiveness, salvation, prayer\"\n                )\n                verses_input = gr.Textbox(\n                    label=\"Specific Verses to Include (optional)\",\n                    placeholder=\"e.g., John 3:16, Romans 3:23\"\n                )\n                study_btn = gr.Button(\"Generate Study\", variant=\"primary\")\n                study_output = gr.Textbox(\n                    label=\"Topic Study\",\n                    lines=15,\n                    interactive=False\n                )\n                \n                study_btn.click(\n                    topic_study,\n                    inputs=[topic_input, verses_input],\n                    outputs=study_output\n                )\n                \n                gr.Examples(\n                    examples=[\n                        \"Love in the New Testament\",\n                        \"Forgiveness and redemption\",\n                        \"The Holy Spirit's role\",\n                        \"Prayer and meditation\",\n                        \"Faith vs works\",\n                        \"The kingdom of heaven\"\n                    ],\n                    inputs=topic_input\n                )\n        \n        gr.Markdown(\n            \"\"\"\n            ---\n            **Model**: `sleepdeprived3/Christian-Bible-Expert-v2.0-12B`\n            \n            **Tips for better results:**\n            - Be specific in your questions\n            - Include verse references when possible\n            - Try different response styles for various perspectives\n            \"\"\"\n        )\n    \n    return demo\n\n# Create and launch the advanced interface\n# advanced_demo = create_advanced_nt_interface()\n# advanced_demo.launch(share=False)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrates multiple approaches to creating a New Testament expert using Hugging Face models:\n\n### Core Methods:\n1. **Pipeline Approach**: Simple text generation for quick questions\n2. **Expert Class**: Structured approach with specialized methods\n3. **Verse Explanation**: Detailed analysis of specific passages\n4. **Gospel Comparison**: Comparative analysis across gospels\n5. **Interactive Session**: Real-time Q&A functionality\n\n### Gradio Chat Interfaces:\n6. **Basic Gradio Chat**: Web-based chat interface with examples\n7. **Multi-Model Comparison**: Compare responses from different biblical models\n8. **Advanced Interface**: Tabbed interface with multiple features including:\n   - Context-aware responses (General, Pastoral, Academic, Devotional)\n   - Adjustable creativity/temperature settings\n   - Dedicated verse lookup tool\n   - Topic study generator\n\n### Available Biblical Models:\n- **sleepdeprived3/Christian-Bible-Expert-v2.0-12B**: Primary biblical expert model\n- **sleepdeprived3/Reformed-Christian-Bible-Expert-12B_EXL2_5bpw_H8**: Reformed/Calvinist perspective\n- **aao331/ChristGPT-13B-V2-GPTQ**: Model trained to act like Jesus\n- **oliverbob/openbible**: Open biblical knowledge model\n- **abhi1nandy2/Bible-roberta-base**: RoBERTa trained on biblical text\n- **Buttsac/bible**: Bible-focused language model\n\n### Key Features:\n- Multiple response styles (pastoral, academic, devotional)\n- Interactive web interfaces with Gradio\n- Model comparison capabilities\n- Verse-specific explanations\n- Topic-based studies\n- Temperature/creativity control\n- Example questions and proper error handling\n\n**Usage**: Uncomment the launch commands in the cells to run the interfaces. Each provides different levels of interaction and functionality for New Testament study and exploration.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}